batch_size: 50
memory_size: 50000
batch_per_learn: 20
init_alpha: 1
target_entropy: 0.2
lr_alpha: 0.001
lr_policy: 0.005
lr_q: 0.001
rho: 0.01
discount: 0.99  # gamma
tune_alpha: True
layers_p: [128]
activation: 'Leaky Relu' #or 'Relu'